{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ec353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "\n",
    "print(physical_devices)\n",
    "#If You have gpu uncomment the line below to speed up the training process\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[1], True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aec25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import gym\n",
    "from gym_pybullet_drones.envs.BaseAviary import DroneModel, Physics\n",
    "from collections import OrderedDict\n",
    "from Base import MyAviary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization\n",
    "from collections import deque\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=18\n",
    "output_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.005            #soft target update\n",
    "gamma = 0.99             #discount factor\n",
    "\n",
    "actor_lr = 1e-4    \n",
    "critic_lr = 1e-4\n",
    "\n",
    "memory_size = int(1e6) \n",
    "minibatch_size = 256\n",
    "\n",
    "num_episodes = 10_000   #max episodes for training\n",
    "num_steps=1_000          #max number of steps in an episode\n",
    "\n",
    "noise_std=0.2      #stddev of noise added for exploration\n",
    "start_after = 2_000     #pure exploration before 1000 steps\n",
    "update_after = 1_000    #start learning after 1000 steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392e736",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tensorboard for monitoring the training process\n",
    "\n",
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = \"logs/train_DDPG/\" + current_time\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {train_log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, input_size, output_size, size):\n",
    "        self.states = np.zeros(shape = (size, input_size), dtype=np.float32)\n",
    "        self.next_states = np.zeros(shape = (size, input_size), dtype=np.float32)\n",
    "        self.actions = np.zeros(shape = (size, output_size), dtype=np.float32)\n",
    "        self.rewards = np.zeros(shape = size, dtype=np.float32)\n",
    "        self.dones = np.zeros(shape = size, dtype=np.float32)\n",
    "        self.pointer=0\n",
    "        self.size=0\n",
    "        self.max_size=size\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.states[self.pointer] = state\n",
    "        self.next_states[self.pointer] = next_state\n",
    "        self.actions[self.pointer] = action\n",
    "        self.rewards[self.pointer] =  reward\n",
    "        self.dones[self.pointer] = done\n",
    "        self.pointer = (self.pointer + 1)%self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "        \n",
    "    def sample(self, minibatch_size = 32):\n",
    "        idxs = np.random.randint(0, self.size, size=minibatch_size)\n",
    "        \n",
    "        return self.states[idxs],\\\n",
    "               self.actions[idxs],\\\n",
    "               self.rewards[idxs],\\\n",
    "               self.next_states[idxs],\\\n",
    "               self.dones[idxs]\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.actor = self.build_actor()\n",
    "        self.critic_1 = self.build_critic()\n",
    "        self.critic_2 = self.build_critic()\n",
    "        \n",
    "        self.target_actor = copy(self.actor)\n",
    "        self.target_critic_1 = copy(self.critic_1)\n",
    "        self.target_critic_2 = copy(self.critic_2)\n",
    "        \n",
    "        self.actor_optimizer = Adam(lr=actor_lr)\n",
    "        self.critic_optimizer =  Adam(lr=critic_lr)\n",
    "        \n",
    "        self.memory = ReplayMemory(input_size, output_size, memory_size)\n",
    "        self.learn_count = 0\n",
    "        self.noise_std = noise_std\n",
    "        self.actor_update_itr = 2\n",
    "        \n",
    "    def build_actor(self):\n",
    "        final_init = tf.random_uniform_initializer(minval=-3e-4, maxval=3e-4)\n",
    "        \n",
    "        inputs = Input(shape = input_size)\n",
    "        x = Dense(400)(inputs)\n",
    "        x = tf.nn.tanh(x)\n",
    "        x = Dense(300)(x)\n",
    "        x = tf.nn.tanh(x)\n",
    "        x = Dense(output_size, kernel_initializer=final_init)(x)\n",
    "        x = tf.nn.tanh(x)\n",
    "        \n",
    "        model = Model(inputs = inputs, outputs = x)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_critic(self):\n",
    "        final_init = tf.random_uniform_initializer(minval=-3e-4, maxval=3e-4)\n",
    "        \n",
    "        input1 = Input(shape = input_size)\n",
    "        input2 = Input(shape = output_size)\n",
    "        x = Dense(400)(tf.concat([input1, input2], axis =1))\n",
    "        x = tf.nn.tanh(x)\n",
    "        x = Dense(300)(x)\n",
    "        x = tf.nn.tanh(x)\n",
    "        x = Dense(1, activation = 'linear', kernel_initializer=final_init)(x)\n",
    "        \n",
    "        model = Model(inputs = [input1, input2], outputs = x)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "        \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    @tf.function\n",
    "    def act(self, state, test=False):\n",
    "        act_value = self.actor(state)\n",
    "        if test:\n",
    "            return act_value\n",
    "        else:\n",
    "            noise = tf.random.normal(shape = act_value.shape, mean=0.0, stddev = self.noise_std)\n",
    "            act_value += tf.clip_by_value(noise, -0.5, 0.5)\n",
    "            return tf.clip_by_value(act_value, -1, 1)\n",
    "                  \n",
    "    @tf.function\n",
    "    def critic_learn(self, states, actions, rewards, next_states, dones):\n",
    "        rewards= tf.expand_dims(rewards, axis=1)\n",
    "        dones = tf.expand_dims(dones, axis=1)\n",
    "      \n",
    "        next_actions = self.target_actor(next_states)\n",
    "        next_actions += tf.clip_by_value(tf.random.normal(shape = next_actions.shape, mean = 0.0, stddev = 0.2), -0.5, 0.5)\n",
    "        next_actions = tf.clip_by_value(next_actions, -1, 1)\n",
    "        next_q_1 = self.target_critic_1([next_states, next_actions])\n",
    "        next_q_2 = self.target_critic_2([next_states, next_actions]) \n",
    "        next_values = tf.math.minimum(next_q_1, next_q_2)\n",
    "        target_q_values = rewards + gamma * (1-dones) * next_values\n",
    "        with tf.GradientTape(persistent=True) as tape:    \n",
    "            pred_q_1 = self.critic_1([states, actions])\n",
    "            pred_q_2 = self.critic_2([states, actions])\n",
    "            critic_loss = tf.reduce_mean((target_q_values - pred_q_1)**2) + tf.reduce_mean((target_q_values - pred_q_2)**2)\n",
    "        critic_1_grads = tape.gradient(critic_loss, self.critic_1.trainable_weights)\n",
    "        critic_2_grads = tape.gradient(critic_loss, self.critic_2.trainable_weights)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_1_grads, self.critic_1.trainable_weights))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_2_grads, self.critic_2.trainable_weights))\n",
    "        return critic_loss\n",
    "\n",
    "    @tf.function\n",
    "    def actor_learn(self, states, actions, rewards, next_states, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions_pred = self.actor(states)\n",
    "            actor_loss = self.critic_1([states, actions_pred])\n",
    "            actor_loss = -tf.reduce_mean(actor_loss)\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_weights)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_weights))\n",
    "        return actor_loss\n",
    "    \n",
    "    def learn(self):\n",
    "        if agent.memory.size<minibatch_size:\n",
    "            return \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(minibatch_size)\n",
    "        critic_loss = self.critic_learn(states, actions, rewards, next_states, dones) \n",
    "        self.learn_count += 1\n",
    "        \n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Charts/critic_loss\", critic_loss, self.learn_count)\n",
    "            \n",
    "        if self.learn_count%self.actor_update_itr != 0:\n",
    "            return\n",
    "        \n",
    "        actor_loss = self.actor_learn(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar(\"Charts/actor_loss\", actor_loss, self.learn_count)\n",
    "            \n",
    "        self.update_target_weights()\n",
    "        \n",
    "    def update_target_weights(self):\n",
    "        target_actor_weights = self.target_actor.get_weights()\n",
    "        weights=[]\n",
    "        for i, weight in enumerate(self.actor.get_weights()):\n",
    "            weights.append(weight * tau + target_actor_weights[i] * (1-tau))\n",
    "        self.target_actor.set_weights(weights)\n",
    "                           \n",
    "        target_critic_1_weights = self.target_critic_1.get_weights()\n",
    "        weights=[]\n",
    "        for i, weight in enumerate(self.critic_1.get_weights()):\n",
    "            weights.append(weight * tau + target_critic_1_weights[i] * (1-tau))\n",
    "        self.target_critic_1.set_weights(weights)\n",
    "        \n",
    "        target_critic_2_weights = self.target_critic_2.get_weights()\n",
    "        weights=[]\n",
    "        for i, weight in enumerate(self.critic_2.get_weights()):\n",
    "            weights.append(weight * tau + target_critic_2_weights[i] * (1-tau))\n",
    "        self.target_critic_2.set_weights(weights)\n",
    "                           \n",
    "    def load(self, name):\n",
    "        self.actor.load_weights(name + \"actor.hdf5\")\n",
    "        self.critic_1.load_weights(name + \"critic_1.hdf5\")\n",
    "        self.critic_2.load_weights(name + \"critic_2.hdf5\")\n",
    "        \n",
    "        self.target_actor.load_weights(name + \"actor.hdf5\")\n",
    "        self.target_critic_1.load_weights(name + \"critic_1.hdf5\")\n",
    "        self.target_critic_2.load_weights(name + \"critic_2.hdf5\")\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.actor.save_weights(name + \"actor.hdf5\")\n",
    "        self.critic_1.save_weights(name + \"critic_1.hdf5\") \n",
    "        self.critic_2.save_weights(name + \"critic_2.hdf5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f46fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_freq = 240\n",
    "control_freq = 48\n",
    "physics_step = int(simulation_freq/control_freq) #In one env.step call, this number of physics steps will be executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aed790",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_l = -1.0\n",
    "xy_h = 1.0\n",
    "z_l = 0.1\n",
    "z_h = 2\n",
    "\n",
    "\n",
    "rpy_l = -8 * np.pi/180\n",
    "rpy_h =  8 * np.pi/180\n",
    "\n",
    "change_target = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773803dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_pos_att():\n",
    "    xy = np.random.uniform(low=xy_l, high=xy_h, size=(1,2))\n",
    "    z = np.ones(shape=(1,1))*z_l\n",
    "    return np.concatenate([xy, z], axis =1), np.random.uniform(low=rpy_l, high=rpy_h, size=(1,3))\n",
    "\n",
    "def generate_target():\n",
    "    \n",
    "    target_xy_l = -1.0\n",
    "    target_xy_h = 1.0\n",
    "    target_z_l = 1\n",
    "    target_z_h = 2\n",
    "    \n",
    "    xy_target = np.random.uniform(low=target_xy_l, high=target_xy_h, size=(2,))\n",
    "    z_target = np.random.uniform(low=target_z_l, high=target_z_h, size=(1,))\n",
    "    return np.concatenate([xy_target, z_target], axis =0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f6ed23",
   "metadata": {},
   "source": [
    "## Training\n",
    "For training, first uncomment the cell below and then run it. It will take some time depending on the machine being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6c2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Training \n",
    "# best_score = -np.inf\n",
    "# best_test_score = -np.inf\n",
    "# score_history = deque(maxlen=100)\n",
    "# position_err = np.inf\n",
    "# global_step = 1\n",
    "\n",
    "# target_position= generate_target()\n",
    "# print(\"Target:\", target_position)\n",
    "\n",
    "\n",
    "# for ep in range(1, 1+num_episodes):\n",
    "    \n",
    "#     INIT_COOR, INIT_ORIENT = init_pos_att()\n",
    "    \n",
    "  \n",
    "#     env = MyAviary(drone_model=DroneModel.CF2X,\n",
    "#                  initial_xyzs=INIT_COOR,\n",
    "#                  initial_rpys=INIT_ORIENT,\n",
    "#                  physics=Physics.PYB,\n",
    "#                  freq = simulation_freq,\n",
    "#                  aggregate_phy_steps=physics_step,\n",
    "#                  gui=False,\n",
    "#                  record=False)       \n",
    "          \n",
    "    \n",
    "    \n",
    "#     env.reset()\n",
    "    \n",
    "#     env._addTarget(target_position, visual = False) \n",
    "    \n",
    "#     state = env._computeObs()\n",
    "#     next_state = state\n",
    "    \n",
    "#     done = False\n",
    "#     score = 0\n",
    "#     step=0\n",
    "\n",
    "#     for step in range(num_steps): \n",
    "#         position_err = np.linalg.norm(next_state[0:3])\n",
    "#         if position_err<0.05:\n",
    "#             with train_summary_writer.as_default():\n",
    "#                 tf.summary.scalar(\"Charts/position_err\", position_err, global_step)\n",
    "            \n",
    "#             target_position = generate_target()\n",
    "#             env._addTarget(target_position, visual = False) \n",
    "#             print(\"New Target:\", target_position)\n",
    "           \n",
    "#         if global_step<start_after:\n",
    "#             action = env.action_space.sample()\n",
    "#         else:\n",
    "#             action = agent.act(np.expand_dims(state, axis = 0))\n",
    "#             action = np.squeeze(action) \n",
    "\n",
    "#         next_state, reward, done, _ =  env.step(action)\n",
    "#         score += reward\n",
    "#         global_step += 1\n",
    " \n",
    "        \n",
    "        \n",
    "#         agent.memory.remember(state, action, reward, next_state, done)\n",
    "#         state = next_state\n",
    "        \n",
    "#         if global_step>update_after:\n",
    "#             agent.learn()\n",
    "            \n",
    "#         if done:            \n",
    "#             env.close()\n",
    "#             break\n",
    "         \n",
    "#     if not done:\n",
    "#         env.close()\n",
    "\n",
    "#     with train_summary_writer.as_default():\n",
    "#         tf.summary.scalar(\"Charts/score\", score, ep)\n",
    "#         tf.summary.scalar(\"Charts/episode_length\", step, ep)\n",
    "#         tf.summary.scalar(\"Charts/exploration\", agent.noise_std, ep)\n",
    "        \n",
    "        \n",
    "#     score_history.append(score)\n",
    "#     avg_score = np.mean(score_history)\n",
    "#     print(\"\\n\")\n",
    "#     print(f\"Episode: {ep}, Len: {step}, Score: {score}, Avg Score: {avg_score}\")  \n",
    "#     print(\"\\n\")\n",
    "#     if avg_score > best_score:\n",
    "#         best_score = score\n",
    "#         agent.save(\"DDPG_\") \n",
    "        \n",
    "#     if ep%25==0:\n",
    "#         agent.save(\"Latest_\")\n",
    "        \n",
    "#         test_target_position= generate_target()\n",
    "#         print(f\"Testing\")\n",
    "#         print(\"Test Target\", test_target_position)\n",
    "#         for _ in range(2):\n",
    "#             print(\"\\n\")\n",
    "            \n",
    "#             INIT_COOR, INIT_ORIENT  = init_pos_att()\n",
    "            \n",
    "#             env = MyAviary(drone_model=DroneModel.CF2X,\n",
    "#                  initial_xyzs=INIT_COOR,\n",
    "#                  initial_rpys=INIT_ORIENT,\n",
    "#                  physics=Physics.PYB,\n",
    "#                  freq = simulation_freq,\n",
    "#                  aggregate_phy_steps=physics_step,\n",
    "#                  gui=False,\n",
    "#                  record=False)\n",
    "            \n",
    "            \n",
    "            \n",
    "#             env.reset()\n",
    "            \n",
    "#             env._addTarget(test_target_position, visual = False)\n",
    "            \n",
    "#             state = env._computeObs()\n",
    "#             next_state = state\n",
    "#             done = False\n",
    "#             test_score=0\n",
    "\n",
    "#             for test_step in range(20_000):\n",
    "\n",
    "#                 if np.linalg.norm(next_state[0:3])<0.05:\n",
    "#                     test_target_position = generate_target()\n",
    "#                     env._addTarget(test_target_position, visual = False)\n",
    "#                     print(\"New Test Target\", test_target_position)\n",
    "                \n",
    "#                 action = agent.act(np.expand_dims(state, axis =0), test=True)\n",
    "#                 next_state, reward, done, _ = env.step(np.squeeze(action))\n",
    "                \n",
    "#                 test_score+= reward\n",
    "                \n",
    "#                 state=next_state\n",
    "                \n",
    "#                 if done:\n",
    "#                     env.close()\n",
    "#                     break\n",
    "                \n",
    "#             print(\"\\n\")\n",
    "#             print(f\"Episode ended with length {test_step} and a score of {test_score}\")\n",
    "#             print(\"\\n\")\n",
    "            \n",
    "#         if test_score>best_test_score:\n",
    "#             best_test_score = test_score\n",
    "#             agent.save(\"BestTest_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd3433",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Run the cells below to test my results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb3e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(\"BestTest_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f562ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    #Testing\n",
    "    time_arr = []\n",
    "    position_err_arr = []\n",
    "    target_position= generate_target()\n",
    "    done = False      \n",
    "    print(f\"Target: {target_position}\")\n",
    "    INIT_COOR, INIT_ORIENT = init_pos_att()\n",
    "    \n",
    "    env = MyAviary(drone_model=DroneModel.CF2X,\n",
    "                 initial_xyzs=INIT_COOR,\n",
    "                 initial_rpys=INIT_ORIENT,\n",
    "                 physics=Physics.PYB,\n",
    "                 freq = simulation_freq,\n",
    "                 aggregate_phy_steps=physics_step,\n",
    "                 gui=True,\n",
    "                 record=True)\n",
    "    \n",
    "    env.reset()\n",
    "    env._addTarget(target_position) \n",
    "    state = env._computeObs()\n",
    "    next_state =  state\n",
    "    \n",
    "   \n",
    "    score=0\n",
    "   \n",
    "    for t in range(1, 25_000):\n",
    "        position_err = np.linalg.norm(next_state[0:3])\n",
    "        \n",
    "                \n",
    "        if position_err<0.06:\n",
    "            \n",
    "            time_arr.append(t)\n",
    "            position_err_arr.append(position_err)\n",
    "            plt.scatter(t, position_err)\n",
    "            print(\"Position Error\", position_err)\n",
    "            target_position = generate_target()    \n",
    "            env._addTarget(target_position)\n",
    "        \n",
    "        action = agent.act(np.expand_dims(state, axis =0), test=True)\n",
    "        next_state, reward, done, _ = env.step(np.squeeze(action))\n",
    "        score+= reward\n",
    "        state = next_state\n",
    "        t += 1\n",
    "        print(f\"Step:{t}, Action:{action}, Position Error:{position_err}, Reward:{reward}\")\n",
    "        if done:\n",
    "            env.close()\n",
    "            break\n",
    "    \n",
    "    print()    \n",
    "    print(f\"Episode ended in {t} steps with score of {score} \\n\")\n",
    "    print()\n",
    "    plt.plot(time_arr, position_err_arr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Position Error\")\n",
    "plt.scatter(time_arr, position_err_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551c3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
