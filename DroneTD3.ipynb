{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ec353",
   "metadata": {
    "id": "d61ec353",
    "outputId": "5a655730-bfc2-4888-9ed1-93eece8605f3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aec25a",
   "metadata": {
    "id": "b2aec25a",
    "outputId": "214298a9-5c49-4de6-8672-c233f34b5e26"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "from copy import copy\n",
    "import gym\n",
    "from gym_pybullet_drones.envs.BaseAviary import DroneModel, Physics\n",
    "from collections import OrderedDict\n",
    "from Base import MyAviary\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization\n",
    "from collections import deque\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0c308",
   "metadata": {
    "id": "cba0c308"
   },
   "outputs": [],
   "source": [
    "input_size=12 + 3\n",
    "output_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f25c9",
   "metadata": {
    "id": "298f25c9"
   },
   "outputs": [],
   "source": [
    "tau = 0.005            #soft target update\n",
    "gamma = 0.99             #discount factor\n",
    "\n",
    "actor_lr = 1e-4     \n",
    "critic_lr = 1e-4\n",
    "\n",
    "memory_size = int(1e6)\n",
    "minibatch_size = 256\n",
    "\n",
    "num_episodes = 10_000   #max episodes for training\n",
    "num_steps=500          #max number of steps in an episode\n",
    "\n",
    "noise_std=0.2       #stddev of noise added for exploration\n",
    "start_after = 1_000     #pure exploration before 1000 steps\n",
    "update_after = 1_000    #start learning after 1000 steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392e736",
   "metadata": {
    "id": "8392e736",
    "outputId": "e1597b94-3716-4389-ee41-38c5162efd8d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# rm -rf ./logs/train_PPO/\n",
    "train_log_dir = \"logs/train_DDPG/\" + current_time\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {train_log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9243a941",
   "metadata": {
    "id": "9243a941"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, input_size, output_size, size):\n",
    "        self.states = np.zeros(shape = (size, input_size), dtype=np.float32)\n",
    "        self.next_states = np.zeros(shape = (size, input_size), dtype=np.float32)\n",
    "        self.actions = np.zeros(shape = (size, output_size), dtype=np.float32)\n",
    "        self.rewards = np.zeros(shape = size, dtype=np.float32)\n",
    "        self.dones = np.zeros(shape = size, dtype=np.float32)\n",
    "        self.pointer=0\n",
    "        self.size=0\n",
    "        self.max_size=size\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.states[self.pointer] = state\n",
    "        self.next_states[self.pointer] = next_state\n",
    "        self.actions[self.pointer] = action\n",
    "        self.rewards[self.pointer] =  reward\n",
    "        self.dones[self.pointer] = done\n",
    "        self.pointer = (self.pointer + 1)%self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "        \n",
    "    def sample(self, minibatch_size = 32):\n",
    "        idxs = np.random.randint(0, self.size, size=minibatch_size)\n",
    "        \n",
    "        return self.states[idxs],\\\n",
    "               self.actions[idxs],\\\n",
    "               self.rewards[idxs],\\\n",
    "               self.next_states[idxs],\\\n",
    "               self.dones[idxs]\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5ee80",
   "metadata": {
    "id": "bfd5ee80"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.actor = self.build_actor()\n",
    "        self.critic_1 = self.build_critic()\n",
    "        self.critic_2 = self.build_critic()\n",
    "        \n",
    "        self.target_actor = copy(self.actor)\n",
    "        self.target_critic_1 = copy(self.critic_1)\n",
    "        self.target_critic_2 = copy(self.critic_2)\n",
    "        \n",
    "        self.actor_optimizer = Adam(lr=actor_lr)\n",
    "        self.critic_optimizer =  Adam(lr=critic_lr)\n",
    "        \n",
    "        self.memory = ReplayMemory(input_size, output_size, memory_size)\n",
    "        self.learn_count = 0\n",
    "        self.noise_std = noise_std\n",
    "        self.actor_update_itr = 2\n",
    "        \n",
    "    def build_actor(self):\n",
    "        final_init = tf.random_uniform_initializer(minval=-3e-4, maxval=3e-4)\n",
    "        init = tf.keras.initializers.HeUniform()\n",
    "        \n",
    "        inputs = Input(shape = input_size)\n",
    "        x = Dense(400, kernel_initializer=init)(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = Dense(300, kernel_initializer=init)(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = Dense(output_size, kernel_initializer=final_init)(x)\n",
    "        x = tf.nn.tanh(x)\n",
    "        model = Model(inputs = inputs, outputs = x)\n",
    "        return model\n",
    "    \n",
    "    def build_critic(self):\n",
    "        final_init = tf.random_uniform_initializer(minval=-3e-4, maxval=3e-4)\n",
    "        init = tf.keras.initializers.HeUniform()\n",
    "        \n",
    "        input1 = Input(shape = input_size)\n",
    "        input2 = Input(shape = output_size)\n",
    "        x = Dense(400, kernel_initializer=init)(tf.concat([input1, input2], axis =1))\n",
    "        x = tf.nn.relu(x)\n",
    "        x = Dense(300, kernel_initializer=init)(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = Dense(1, activation = 'linear', kernel_initializer=final_init)(x)\n",
    "        model = Model(inputs = [input1, input2], outputs = x)\n",
    "        return model\n",
    "    \n",
    "        \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    @tf.function\n",
    "    def act(self, state, test=False):\n",
    "        act_value = self.actor(state)\n",
    "        if test:\n",
    "            return act_value\n",
    "        else:\n",
    "            noise = tf.random.normal(shape = act_value.shape, mean=0.0, stddev = self.noise_std)\n",
    "            act_value += tf.clip_by_value(noise, -0.5, 0.5)\n",
    "            return tf.clip_by_value(act_value, -1, 1)\n",
    "                  \n",
    "   \n",
    "    \n",
    "    @tf.function\n",
    "    def critic_learn(self, states, actions, rewards, next_states, dones):\n",
    "        rewards= tf.expand_dims(rewards, axis=1)\n",
    "        dones = tf.expand_dims(dones, axis=1)\n",
    "      \n",
    "        next_actions = self.target_actor(next_states)\n",
    "        next_actions += tf.clip_by_value(tf.random.normal(shape = next_actions.shape, mean = 0.0, stddev = 0.2), -0.5, 0.5)\n",
    "        next_actions = tf.clip_by_value(next_actions, -1, 1)\n",
    "        \n",
    "        next_q_1 = self.target_critic_1([next_states, next_actions])\n",
    "       \n",
    "        next_q_2 = self.target_critic_2([next_states, next_actions]) \n",
    "      \n",
    "        next_values = tf.math.minimum(next_q_1, next_q_2)\n",
    "      \n",
    "        target_q_values = rewards + gamma * (1-dones) * next_values\n",
    "      \n",
    "        with tf.GradientTape(persistent=True) as tape:  \n",
    "            \n",
    "              \n",
    "            pred_q_1 = self.critic_1([states, actions])\n",
    "            pred_q_2 = self.critic_2([states, actions])\n",
    "            \n",
    "            critic_loss = tf.reduce_mean((target_q_values - pred_q_1)**2) + tf.reduce_mean((target_q_values - pred_q_2)**2)\n",
    "            \n",
    "        critic_1_grads = tape.gradient(critic_loss, self.critic_1.trainable_weights)\n",
    "        critic_2_grads = tape.gradient(critic_loss, self.critic_2.trainable_weights)\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(zip(critic_1_grads, self.critic_1.trainable_weights))\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_2_grads, self.critic_2.trainable_weights))\n",
    "        \n",
    "\n",
    "    @tf.function\n",
    "    def actor_learn(self, states, actions, rewards, next_states, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions_pred = self.actor(states)\n",
    "            actor_loss = self.critic_1([states, actions_pred])\n",
    "            actor_loss = -tf.reduce_mean(actor_loss)\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_weights)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_weights))\n",
    "    \n",
    "    def learn(self):\n",
    "        if agent.memory.size<minibatch_size:\n",
    "            return \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(minibatch_size)\n",
    "        self.critic_learn(states, actions, rewards, next_states, dones) \n",
    "        \n",
    "        self.learn_count += 1\n",
    "        \n",
    "        if self.learn_count%self.actor_update_itr != 0:\n",
    "            return\n",
    "        \n",
    "        self.actor_learn(states, actions, rewards, next_states, dones)\n",
    "        self.update_target_weights()\n",
    "        \n",
    "    def update_target_weights(self):\n",
    "        target_actor_weights = self.target_actor.get_weights()\n",
    "        weights=[]\n",
    "        for i, weight in enumerate(self.actor.get_weights()):\n",
    "            weights.append(weight * tau + target_actor_weights[i] * (1-tau))\n",
    "        self.target_actor.set_weights(weights)\n",
    "        \n",
    "                           \n",
    "        target_critic_1_weights = self.target_critic_1.get_weights()\n",
    "        weights=[]\n",
    "        for i, weight in enumerate(self.critic_1.get_weights()):\n",
    "            weights.append(weight * tau + target_critic_1_weights[i] * (1-tau))\n",
    "        self.target_critic_1.set_weights(weights)\n",
    "        \n",
    "        target_critic_2_weights = self.target_critic_2.get_weights()\n",
    "        weights=[]\n",
    "        for i, weight in enumerate(self.critic_2.get_weights()):\n",
    "            weights.append(weight * tau + target_critic_2_weights[i] * (1-tau))\n",
    "        self.target_critic_2.set_weights(weights)\n",
    "                           \n",
    "\n",
    "    def load(self, name):\n",
    "        self.actor.load_weights(name + \"actor.hdf5\")\n",
    "        self.critic_1.load_weights(name + \"critic_1.hdf5\")\n",
    "        self.critic_2.load_weights(name + \"critic_2.hdf5\")\n",
    "        \n",
    "        self.target_actor.load_weights(name + \"actor.hdf5\")\n",
    "        self.target_critic_1.load_weights(name + \"critic_1.hdf5\")\n",
    "        self.target_critic_2.load_weights(name + \"critic_2.hdf5\")\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.actor.save_weights(name + \"actor.hdf5\")\n",
    "        self.critic_1.save_weights(name + \"critic_1.hdf5\") \n",
    "        self.critic_2.save_weights(name + \"critic_2.hdf5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f46fd5",
   "metadata": {
    "id": "d8f46fd5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85e2bf",
   "metadata": {
    "id": "ed85e2bf"
   },
   "outputs": [],
   "source": [
    "simulation_freq = 50\n",
    "control_freq = 50\n",
    "physics_step = int(simulation_freq/control_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aed790",
   "metadata": {
    "id": "49aed790"
   },
   "outputs": [],
   "source": [
    "xy_l = -1.0\n",
    "xy_h = 1.0\n",
    "z_l = 0.2\n",
    "z_h = 2\n",
    "\n",
    "\n",
    "rpy_l = -8 * np.pi/180\n",
    "rpy_h =  8 * np.pi/180\n",
    "\n",
    "change_target = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773803dc",
   "metadata": {
    "id": "773803dc"
   },
   "outputs": [],
   "source": [
    "def init_pos_att():\n",
    "    xy = np.random.uniform(low=xy_l, high=xy_h, size=(1,2))\n",
    "    z = np.ones(shape=(1,1))*z_l\n",
    "    return np.concatenate([xy, z], axis =1), np.random.uniform(low=rpy_l, high=rpy_h, size=(1,3))\n",
    "\n",
    "def generate_target():\n",
    "    \n",
    "    target_xy_l = -1.0\n",
    "    target_xy_h = 1.0\n",
    "    target_z_l = 1\n",
    "    target_z_h = 2\n",
    "    \n",
    "    xy_target = np.random.uniform(low=target_xy_l, high=target_xy_h, size=(2,))\n",
    "    z_target = np.random.uniform(low=target_z_l, high=target_z_h, size=(1,))\n",
    "    return np.concatenate([xy_target, z_target], axis =0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4f81a",
   "metadata": {},
   "source": [
    "## Training\n",
    "#### Run the cell below to initiate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6c2cf",
   "metadata": {
    "id": "ddc6c2cf",
    "outputId": "ccd031db-ef0e-4081-cd54-82c4f8ce33a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training \n",
    "best_score = -np.inf\n",
    "best_test_score = -np.inf\n",
    "score_history = deque(maxlen=100)\n",
    "\n",
    "global_step = 1\n",
    "\n",
    "target_position= generate_target()\n",
    "\n",
    "\n",
    "\n",
    "for ep in range(1, 1+num_episodes):\n",
    "    \n",
    "    INIT_COOR, INIT_ORIENT = init_pos_att()\n",
    "    \n",
    "  \n",
    "    env = MyAviary(drone_model=DroneModel.CF2X,\n",
    "                 initial_xyzs=INIT_COOR,\n",
    "                 initial_rpys=INIT_ORIENT,\n",
    "                 physics=Physics.PYB,\n",
    "                 freq = simulation_freq,\n",
    "                 aggregate_phy_steps=physics_step,\n",
    "                 gui=False,\n",
    "                 record=False)       \n",
    "          \n",
    "    print(f\"Target: {target_position}\")\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    env._addTarget(target_position, visual = False) \n",
    "    \n",
    "    state = env._computeObs()\n",
    "    \n",
    "    done = False\n",
    "    score = 0\n",
    "    step=0\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    for step in range(num_steps): \n",
    "        if global_step%500==0:\n",
    "            target_position = generate_target()\n",
    "            env._addTarget(target_position, visual = False) \n",
    "           \n",
    "        if global_step<start_after:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.act(np.expand_dims(state, axis = 0))\n",
    "            action = np.squeeze(action) \n",
    "            \n",
    "        next_state, reward, done, _ =  env.step(action)\n",
    "        score += reward\n",
    "        global_step += 1\n",
    "        \n",
    "        agent.memory.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if global_step>update_after:\n",
    "            agent.learn()\n",
    "            \n",
    "        if done:            \n",
    "            env.close()\n",
    "            break\n",
    "         \n",
    "    if not done:\n",
    "        env.close()\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"Charts/score\", score, ep)\n",
    "        tf.summary.scalar(\"Charts/episode_length\", step, ep)\n",
    "        tf.summary.scalar(\"Charts/exploration\", agent.noise_std, ep)\n",
    "        \n",
    "        \n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Episode: {ep}, Len: {step}, Score: {score}, Avg Score: {avg_score}\")  \n",
    "    print(\"\\n\")\n",
    "    if avg_score > best_score:\n",
    "        best_score = score\n",
    "        agent.save(\"DDPG_\") \n",
    "        \n",
    "    if ep%25==0:\n",
    "        agent.save(\"Latest_\")\n",
    "        \n",
    "        test_target_position= generate_target()\n",
    "        print(f\"Testing\")\n",
    "        \n",
    "        for _ in range(2):\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            INIT_COOR, INIT_ORIENT  = init_pos_att()\n",
    "            \n",
    "            env = MyAviary(drone_model=DroneModel.CF2X,\n",
    "                 initial_xyzs=INIT_COOR,\n",
    "                 initial_rpys=INIT_ORIENT,\n",
    "                 physics=Physics.PYB,\n",
    "                 freq = simulation_freq,\n",
    "                 aggregate_phy_steps=physics_step,\n",
    "                 gui=False,\n",
    "                 record=False)\n",
    "            \n",
    "            print(f\"Target: {test_target_position}\")\n",
    "            \n",
    "            env.reset()\n",
    "            \n",
    "            env._addTarget(test_target_position, visual = False)\n",
    "            \n",
    "            state = env._computeObs()\n",
    "            done = False\n",
    "            test_score=0\n",
    "\n",
    "            for test_step in range(20_000):\n",
    "\n",
    "                if test_step%1_000==0:\n",
    "                    test_target_position = generate_target()\n",
    "                    env._addTarget(test_target_position, visual = False)\n",
    "                \n",
    "                action = agent.act(np.expand_dims(state, axis =0), test=True)\n",
    "                next_state, reward, done, _ = env.step(np.squeeze(action))\n",
    "                \n",
    "                test_score+= reward\n",
    "                \n",
    "                state=next_state\n",
    "                \n",
    "                if done:\n",
    "                    env.close()\n",
    "                    break\n",
    "                \n",
    "            print(\"\\n\")\n",
    "            print(f\"Episode ended with length {test_step} and a score of {test_score}\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        if test_score>best_test_score:\n",
    "            best_test_score = test_score\n",
    "            agent.save(\"BestTest_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29552ff6",
   "metadata": {
    "id": "daefc9b1"
   },
   "source": [
    "# Testing\n",
    "#### Run all the cells below to initiate testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9df47",
   "metadata": {
    "id": "bfa9df47"
   },
   "outputs": [],
   "source": [
    "INIT_COOR, INIT_ORIENT = init_pos_att()\n",
    "target_position = generate_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a34e8",
   "metadata": {
    "id": "cd6a34e8",
    "outputId": "0fd7f63f-5ada-4b7e-e5d1-ba471b26fd7c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INIT_COOR, INIT_ORIENT, target_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875fb1a6",
   "metadata": {
    "id": "875fb1a6",
    "outputId": "e2bfb536-7554-41bd-d870-540d55ad4033",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = MyAviary(drone_model=DroneModel.CF2X,\n",
    "                 initial_xyzs=INIT_COOR,\n",
    "                 initial_rpys=INIT_ORIENT,\n",
    "                 physics=Physics.PYB,\n",
    "                 freq = simulation_freq,\n",
    "                 aggregate_phy_steps=physics_step,\n",
    "                 gui=True,\n",
    "                 record=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eae21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(\"BestTest_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f562ee5",
   "metadata": {
    "id": "0f562ee5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "\n",
    "target_position= generate_target()\n",
    "\n",
    "for _ in range(10):\n",
    "    done = False      \n",
    "    print(f\"Target: {target_position}\")\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    env._addTarget(target_position) \n",
    "    \n",
    "    state = env._computeObs()\n",
    "    \n",
    "   \n",
    "    score=0\n",
    "   \n",
    "    for t in range(1, 50000):      \n",
    "        if t%1000==0:\n",
    "            target_position = generate_target()    \n",
    "            env._addTarget(target_position)\n",
    "        \n",
    "        action = agent.act(np.expand_dims(state, axis =0), test=False)\n",
    "        next_state, reward, done, _ = env.step(np.squeeze(action))\n",
    "        score+= reward\n",
    "        state = next_state\n",
    "        t += 1\n",
    "        print(f\"Step:{t}, Action:{action}\")\n",
    "        if done:\n",
    "        \n",
    "            break\n",
    "    print()    \n",
    "    print(f\"Episode ended in {t} steps with score of {score} \\n\")\n",
    "    print()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da721c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DronesDDPGTD3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
